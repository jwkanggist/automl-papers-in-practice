# Simple and Scalable Predictive Uncertainty Estimation using Deep Ensemble

Paper Authors: Balaji Lakshminarayanan et al.

Affiliation: Google DeepMind

Publication: NIPS2017 ([`NIPS video`](https://www.facebook.com/nipsfoundation/videos/1554654864625747/), [`ArXiv URL`](https://arxiv.org/abs/1612.01474), [`Github URL`](https://github.com/Kyushik/Predictive-Uncertainty-Estimation-using-Deep-Ensemble))
- ref blog: https://tech.instacart.com/3-nips-papers-we-loved-befb39a75ec2

Update date: Marco @ 191112

---
> ì˜¤íƒ€ì£¼ì˜ (ë°œí‘œìì—ê²ŒëŠ” ê´€ëŒ€í•©ì‹œë‹¤) 

### 1. Key Question and Hypothesis of Present Paper
- ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ì•„ë˜ì— ë„ˆë¬´ ë¯¼ê° í•˜ê³  ê°™ì€ ëª¨ë¸ + ë°ì´í„°ë¼ í•˜ë”ë¼ë„ ì˜ˆì¸¡ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
    - seen data ì™€ unseen data
    - weight ì´ˆê¸°ê°’ 
    - í•˜ì´í¼ íŒŒë¼ë¯¸í„° ë³€ê²½

- Predictive uncertainty estimation
```
Simple and Scalable Predictive Uncertainty Estimation using Deep Ensemble
```
    - ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ í™•ì‹¤í•œì§€ ì¸¡ì • í•  ìˆ˜ ìˆëŠ”ê°€?
    - ë¶ˆí™•ì‹¤ì„± (predictive uncertainty)ì„ ì¸¡ì •í•˜ê³  ìš°ë¦¬ê°€ ì œì–´ í•  ìˆ˜ ìˆëŠ”ê°€?
    - ë°ì´í„°ì…‹ì˜ ë„ë©”ì¸ ì‹œí”„íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë„ ë¶ˆí™•ì‹¤ì„±ì´ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ê°€? ê·¸ê²ƒì„ ì•Œê³  ì œì–´ í•  ìˆ˜ ìˆëŠ”ê°€?


### 2. Main Contributions
- 1) `predictive uncertainty estimation` ì´ë¼ëŠ” ê°œë…ì„ ì²˜ìŒìœ¼ë¡œ ì œì‹œ
    - ì ìš© í•  ìˆ˜ ìˆëŠ” simple pipeline ì œì‹œ
        - proper scoring rule: íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” lossí•¨ìˆ˜
        - adversarial training    

- 2) ìœ„ ë‘ ê°€ì§€ë¥¼ ì´ìš©í•œ ì•ˆì •ëœ ì˜ˆì¸¡(smooth prediction)ì„ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡  ì œì‹œ
    - uniformly-weighted mixture ensemble
    - ê²°ê³¼ë¥¼ ì•™ìƒë¸” í•˜ì§€ë§ê³  distributionì„ ì•™ìƒë¸” í•˜ì
        - distributionì„ ì˜ ëª¨ë¸ë§ í•´ì•¼í•¨ : í”í•˜ê²Œ gaussian / mixture gaussian ì‚¬ìš©
    - classification / regression ëª¨ë‘ ì ìš©ê°€ëŠ¥

- 3) low-computation and simple modification to classical NN training pipeline
    - variational inference / MCMC ê¸°ë°˜ì˜ Bayesian NN ë³´ë‹¤ ë§¤ìš° simple

- ì•„ë˜ëŠ” ì €ìì˜ NIPS2017 ë°œí‘œìë£Œë¡œ ë¶€í„° ë°œì·Œ
<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/contribution.png" title="contrib">
</p>

### 3. Related works

#### Bayesian NNs
- íŒŒë¼ë¯¸í„° ë“¤ì— ëŒ€í•œ priorì„ ê°€ì •í•˜ê³  posterior ì„ êµ¬í•´ì„œ regulation í•˜ëŠ” ë°©ì‹
- prior ì„ ì •í™•í•˜ê²Œ ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”
- Bayesian network ë¥¼ ë³µì¡í•˜ê²Œ ì„¤ê³„í•  ìˆ˜ë¡ ê³„ì‚°ëŸ‰ì´ í¬ë‹¤ 
- ì¼ë°˜ì ìœ¼ë¡œ computational relaxation ì„ ì ìš©í•˜ì—¬ êµ¬í˜„
    - model approximation (parameteric ë°©ì‹ - variational inferencen ê³„ì—´)
    - sampling (nonparametric ë°©ì‹ - MCMC ê³„ì—´)
-  ì ì ˆí•œ computational relaxation ë°©ë²•ì„ ëª»ì°¾ëŠ” ê²½ìš° ì‹¤ìš©í™” ì–´ë ¤ì›€   

#### Monte carlo dropout (baseline)
- dropoutì´ ê¸°ë³¸ì ìœ¼ë¡œ model ensemble combination ì´ë¼ëŠ” ê²ƒì— ì£¼ëª©í•´ì„œ í•˜ëŠ”ê±´ë° 
- dropout í•˜ë“¯ì´  modelì„ samplingí•´ì„œ í›ˆë ¨ í›„ ensemble. model samplingì— MC ì ìš©
- ì•„ì§ ì•ˆì½ì–´ ë´„


### 4. Method Summary

#### Deep ensemble 

##### 1) scoring rule
- S(p_theta,(y,x))ë¡œ í‘œê¸°; predictive distribution, p_theta, ì˜ í•¨ìˆ˜
- measure the quality of predictive uncertainty (ë†’ì„ ìˆ˜ë¡ uncertainty ë‚®ìŒ)
- scoring ruleì„ true distributionë¡œ expectation í•œ ê²ƒì„ ì•„ë˜ì™€ ê°™ì´ ì •ì˜

<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/scoring-rule.png" title="scoring-rule">
</p>
    - p_theta : predictive distribution
    - q: true distribution (given by training set)

- **A proper scoring rule** : ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°
<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/proper-scoring-rule.png" title="proper-scoring-rule">
</p>

- (-)ë¥¼ ë¶™ì—¬ì„œ lossë¡œ ì‚¬ìš©

- 1) proper scoring rule for classification :
    - `softmax loss`: S(p_theta,(y,x)) = log p(y|x) ì´ê³  k-multi classification ë¬¸ì œ ì¸ ê²½ìš°
    - `Brier score`: one-hotê³¼ pred distì‚¬ì´ì˜ cross entropyê°€ ì•„ë‹Œ MSE lossí•¨ìˆ˜ë¥¼ êµ¬ì„±

- 2) proper scoring rule for regression:
    - `negative log likelihood (NLL)`: estimating **mean** and **variance** both
    - `MSE` seeks to only estimate the mean 

<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/equ1-nll.png" title="equ1-nll">
</p>

- tips:
    - gaussianì€ ììœ ë„ê°€ ë‚®ìŒ: the other mixture gaussian is better
    - MAP with proper prior can be better than prior

- `remark`: ìš”ì§€ëŠ” model output ensemble í•˜ì§€ë§ê³  predictive distributionì„ parametricìœ¼ë¡œ ì˜ ëª¨ë¸ë§í•´ì„œ distribution ensembleí•´ì•¼í•œë‹¤


##### 2) adversarial training (AT)
- ëª©ì : smoothing predictive distribution
    - model outputë§Œ ì¶œë ¥í•˜ëŠ” ê²ƒì€ uncertainty ì¸¡ì •ì„ ì „í˜€ í•  ìˆ˜ ì—†ìŒ
    - ì•½ê°„ì˜ lossê°€ ì¦ê°€í•˜ëŠ” ë°©í–¥ì˜ ë…¸ì´ì¦ˆë¥¼ ì£¼ì–´ì„œ ëª¨ë¸ì„ ê°•ì¸í•˜ê²Œ ë§Œë“¤ê³  ì˜ í•™ìŠµì‹œí‚´
- training data x ë¡œ ë¶€í„° lossê°€ ì¦ê°€í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ perturbationì„ ë”í•´ì„œ augmentationì„ í•˜ëŠ” ë°©ë²•
- ex) **fast gradient sign** 
    
<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/fast-gradient-sign.png" title="fast-gradient-sign">
</p>

- ATëŠ” í•­ìƒ lossê°€ ì¦ê°€í•˜ëŠ” augmentationì„ ë³´ì¥
- ì£¼ì–´ì§„ ë°ì´í„° ì£¼ìœ„ /epsilon ë°˜ê²½ìœ¼ë¡œ likelihoodë¥¼ í™•ëŒ€ 
    - encourage p(y|x) to be similar to p(y| x + /epsilon)
- model prediction coverageë¥¼ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í™•ëŒ€
```
Interestingly, adversarial training can be interpreted as a computationally efficient solution to smooth
the predictive distributions by increasing the likelihood of the target around an /epsilon-neighborhood of
the observed training example
```
 
 - random direction: x' = x + random ìœ¼ë¡œ augmentationì„ í•  ìˆ˜ ë„ ìˆìœ¼ë‚˜ lossì˜ ì¦ê°€ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŒ

##### 3) uniformly-weighted mixture ensemble
- ensembleì€ í¬ê²Œ random forestê³„ì—´ê³¼ boostingê³„ì—´ë¡œ ë‚˜ë‰¨
    - parallelizationì´ ì‰½ë‹¤ëŠ” ì¸¡ë©´ì—ì„œ random forest ì„ í˜¸; boostingê³„ì—´ì€ multiple optimaê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë™ì‘ì„ ì˜ ì•ˆí•´ì„œ deep learningì— ë§ì§€ ì•ŠìŒ
    - random forestì˜ ì•½ì ì€ branchë§ˆë‹¤ì˜ correlationì´ ì»¤ì§€ëŠ”ê²ƒ
    - ê³³ê³³ì— randomizationì„ í•˜ì—¬ de-correlation í•˜ëŠ” ê²ƒì´ ì¤‘ìš”
        - random init
        - random suffling before data batch builiding
        
- parallelí•˜ê²Œ í•™ìŠµí•´ì„œ **uniformly-weighted mixture ensemble** ì‚¬ìš©
    
<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/uniformly-weight-mixture.png" title="uniformly-weight-mixture">
</p>


    
<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/algo.png" title="algo">
</p>

- ê° ensemble branchë¥¼ ìœ„í•œ data random sampliing
- **fast gradient sign** ë¡œ adversarial augmentation
- loss forr adversarial learning
- uniformly-weighted mixture ensemble for M models

### 5. Experimental Result

```
- batch size 100
- adam opt
- lr 0.1 fixed
- \epsilon =0.01 for AT
```
#### 1) toy example : y = x^3 + noise 

- deep ensemble ë°©ë²•ì´ ì–¼ë§ˆë‚˜ predictive uncertainty estimation ì„ ì˜í•˜ëŠ”ì§€ ë³´ì—¬ì£¼ê¸° ìœ„í•¨
    - NLL + AT + ensemble ì´ ì˜í•œë‹¤

    
<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/fig1.png" title="fig1">
</p>

- left most: MSE pred + ensemble M=5
- second left : NLL pred (est mean and var) + ensemble M=1
- second right : NLL pred (est mean and var) + ensemble M=1 + Adversarial training
- right most : NLL pred (est mean and var) + ensemble M=5 + Adversarial training

    - gray : mean + 3 sigma
    - red : noisy observation
    - blue : true


#### 2) Classification deon MNIST, SVHN, and ImageNet


##### MNIST
```
- model 1: MLP with 3-hidden layers with 200 hidden units
```

<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/fig2-a.png" title="fig2-a">
</p>

- M ì¦ê°€ì— ë”°ë¼ì„œ ì„±ëŠ¥ ê°œì„  
- MC dropout ë³´ë‹¤ ì˜í•¨
- AT ê°€ random aug(R) ë³´ë‹¤ ì˜í•¨
- 2 layer MLP / CNNì—ì„œë„ ë¹„ìŠ·í•œ ë™ì‘ í™•ì¸


##### SVHN

<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/fig2-b.png" title="fig2-b">
</p>

- MNIST ì— ë¹„í•´ì„œ íš¨ê³¼ê°€ ì—†ìŒ 
- class ê°„ dataì˜ íŠ¹ì§•ì´ ëšœë ·íˆ êµ¬ë¶„ë˜ëŠ” ë°ì´í„° ì…‹ì—ì„œëŠ” íš¨ê³¼ê°€ ì—†ë‹¤ê³  í•¨

##### ImageNet
- M ì¦ê°€ì— ë”°ë¼ì„œ ì„±ëŠ¥ ê°œì„  


<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/fig4.png" title="fig4">
</p>


#### 3) Uncertainty evaluion: test example from know vs unknow classes

- unseen data (out-of-distribution data)ì˜ predictive uncertainty ì¸¡ì • ëª©ì 
- training dataì— í¬í•¨ë˜ì§€ ì•Šì€ ë˜ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ë°ì´í„° ì…‹ì€ ë†’ì€ uncertaintyë¥¼ ê°€ì§€ëŠ”ê²Œ desirable í•¨


##### 1 MNIST train + NotMNIST test (alphabat in MNIST format)
    
<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/fig3-a.png" title="fig3-a">
</p>

- entropy histogram
- known class (MNIST test data) ëŠ” entropyê°€ ë§¤ìš° ë‚®ìŒ (0ì— ëª°ë¦¼)
- unknown class (NotMIST test data)ì—ì„œëŠ” entropy  ê°€ í¬ë‹¤
    - Mì´ ì»¤ì§ˆ ìˆ˜ë¡ entropyê°€ í¼ (ë¶ˆí™•ì‹¤ì„±ì´ í¬ë‹¤)
    - MC-dropout ë„ ì»¤ì§€ê¸°ëŠ” í•˜ë‚˜ modeëŠ” ì—¬ì „íˆ 0ì´ë‹¤
    - ATê°€ ì œì¼ ë¹¨ë¦¬ entropy ì¦

##### 2 SVHN train + CIFAR test (SVHN not incluidng digit images)
- ë¹„ìŠ·í•œ ê²½í–¥

    
<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/fig3-b.png" title="fig3-b">
</p>

##### 3 ImageNet dog image train + non-=dog image test
- spliting train set by categories
- train by the dog set
- test by non-dog set 


<p align="center">
  <img src="https://github.com/jwkanggist/automl-papers-in-practice/blob/master/share-reports/figs/deep-ensemble/fig5.png" title="fig5">
</p>


### 6. Discussion


### ê±´ì§ˆë§Œí•œ ê²ƒë“¤

- 

